{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from functions import custom_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './DSL_Winter_Project_2024/'\n",
    "df_without_otiliers_pca = pd.read_csv(path + 'train_without_outliers_pca.csv')\n",
    "df_without_scaler=pd.read_csv(path + 'train_without_outliers_scaled.csv')\n",
    "df_without_pca=pd.read_csv(path + 'train_without_outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC66</th>\n",
       "      <th>PC67</th>\n",
       "      <th>PC68</th>\n",
       "      <th>PC69</th>\n",
       "      <th>PC70</th>\n",
       "      <th>PC71</th>\n",
       "      <th>PC72</th>\n",
       "      <th>PC73</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.737486</td>\n",
       "      <td>-2.731021</td>\n",
       "      <td>1.003991</td>\n",
       "      <td>-0.749037</td>\n",
       "      <td>0.343120</td>\n",
       "      <td>0.137300</td>\n",
       "      <td>-0.636270</td>\n",
       "      <td>1.110771</td>\n",
       "      <td>0.716239</td>\n",
       "      <td>-0.910031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.434828</td>\n",
       "      <td>0.097037</td>\n",
       "      <td>-0.538475</td>\n",
       "      <td>-0.391294</td>\n",
       "      <td>0.255213</td>\n",
       "      <td>-0.158027</td>\n",
       "      <td>0.119743</td>\n",
       "      <td>0.068547</td>\n",
       "      <td>425.0</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.065744</td>\n",
       "      <td>-2.786316</td>\n",
       "      <td>2.362573</td>\n",
       "      <td>0.100823</td>\n",
       "      <td>-3.157451</td>\n",
       "      <td>-0.773902</td>\n",
       "      <td>-0.613166</td>\n",
       "      <td>1.171056</td>\n",
       "      <td>-1.146384</td>\n",
       "      <td>0.220550</td>\n",
       "      <td>...</td>\n",
       "      <td>1.305885</td>\n",
       "      <td>0.227704</td>\n",
       "      <td>0.129430</td>\n",
       "      <td>0.120253</td>\n",
       "      <td>0.704566</td>\n",
       "      <td>0.516766</td>\n",
       "      <td>-0.532212</td>\n",
       "      <td>0.085923</td>\n",
       "      <td>575.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.150287</td>\n",
       "      <td>-4.763248</td>\n",
       "      <td>-2.610161</td>\n",
       "      <td>-2.203354</td>\n",
       "      <td>0.662713</td>\n",
       "      <td>-0.600236</td>\n",
       "      <td>-2.152416</td>\n",
       "      <td>-0.909347</td>\n",
       "      <td>-0.245220</td>\n",
       "      <td>1.599408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407897</td>\n",
       "      <td>-0.104403</td>\n",
       "      <td>-0.314425</td>\n",
       "      <td>0.310318</td>\n",
       "      <td>0.068415</td>\n",
       "      <td>-0.420980</td>\n",
       "      <td>-0.171047</td>\n",
       "      <td>-0.072021</td>\n",
       "      <td>245.0</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.861710</td>\n",
       "      <td>4.349421</td>\n",
       "      <td>-0.142349</td>\n",
       "      <td>0.402060</td>\n",
       "      <td>-1.094392</td>\n",
       "      <td>6.480412</td>\n",
       "      <td>-1.559656</td>\n",
       "      <td>-0.020604</td>\n",
       "      <td>-1.249346</td>\n",
       "      <td>-1.369321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687174</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>1.320186</td>\n",
       "      <td>-0.667891</td>\n",
       "      <td>0.400407</td>\n",
       "      <td>-0.347652</td>\n",
       "      <td>-0.463541</td>\n",
       "      <td>1.118421</td>\n",
       "      <td>395.0</td>\n",
       "      <td>555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.101982</td>\n",
       "      <td>1.267833</td>\n",
       "      <td>2.009555</td>\n",
       "      <td>-0.822039</td>\n",
       "      <td>0.393089</td>\n",
       "      <td>0.163441</td>\n",
       "      <td>0.653145</td>\n",
       "      <td>0.437987</td>\n",
       "      <td>1.574366</td>\n",
       "      <td>-0.093781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168010</td>\n",
       "      <td>-0.163811</td>\n",
       "      <td>0.932782</td>\n",
       "      <td>0.929322</td>\n",
       "      <td>-0.015037</td>\n",
       "      <td>-1.069594</td>\n",
       "      <td>-0.199652</td>\n",
       "      <td>-0.251304</td>\n",
       "      <td>275.0</td>\n",
       "      <td>490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267606</th>\n",
       "      <td>4.308645</td>\n",
       "      <td>0.943745</td>\n",
       "      <td>0.173160</td>\n",
       "      <td>0.638737</td>\n",
       "      <td>-0.904907</td>\n",
       "      <td>-1.192973</td>\n",
       "      <td>2.258960</td>\n",
       "      <td>0.121981</td>\n",
       "      <td>-1.075240</td>\n",
       "      <td>-1.107762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055056</td>\n",
       "      <td>-0.511925</td>\n",
       "      <td>0.015117</td>\n",
       "      <td>0.091863</td>\n",
       "      <td>-0.000452</td>\n",
       "      <td>0.407204</td>\n",
       "      <td>0.567554</td>\n",
       "      <td>-0.640532</td>\n",
       "      <td>580.0</td>\n",
       "      <td>390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267607</th>\n",
       "      <td>4.802572</td>\n",
       "      <td>-2.544687</td>\n",
       "      <td>2.720127</td>\n",
       "      <td>2.083392</td>\n",
       "      <td>-3.946778</td>\n",
       "      <td>-3.476563</td>\n",
       "      <td>0.308218</td>\n",
       "      <td>0.347116</td>\n",
       "      <td>-1.853717</td>\n",
       "      <td>5.699646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450210</td>\n",
       "      <td>0.233386</td>\n",
       "      <td>1.010854</td>\n",
       "      <td>-0.238552</td>\n",
       "      <td>0.103711</td>\n",
       "      <td>1.291865</td>\n",
       "      <td>-0.655887</td>\n",
       "      <td>-0.371007</td>\n",
       "      <td>580.0</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267608</th>\n",
       "      <td>-2.871135</td>\n",
       "      <td>3.626576</td>\n",
       "      <td>1.294119</td>\n",
       "      <td>0.568461</td>\n",
       "      <td>-0.734781</td>\n",
       "      <td>-0.577068</td>\n",
       "      <td>-0.642085</td>\n",
       "      <td>0.240285</td>\n",
       "      <td>-0.551313</td>\n",
       "      <td>-0.299072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670614</td>\n",
       "      <td>0.043020</td>\n",
       "      <td>0.249646</td>\n",
       "      <td>0.551378</td>\n",
       "      <td>0.226044</td>\n",
       "      <td>0.939201</td>\n",
       "      <td>-0.258860</td>\n",
       "      <td>-0.573653</td>\n",
       "      <td>360.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267609</th>\n",
       "      <td>-0.552108</td>\n",
       "      <td>-1.180191</td>\n",
       "      <td>0.466246</td>\n",
       "      <td>1.071564</td>\n",
       "      <td>2.974300</td>\n",
       "      <td>-0.606148</td>\n",
       "      <td>-1.863909</td>\n",
       "      <td>-2.358189</td>\n",
       "      <td>1.045945</td>\n",
       "      <td>-0.040700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140421</td>\n",
       "      <td>-0.528426</td>\n",
       "      <td>0.756357</td>\n",
       "      <td>0.186438</td>\n",
       "      <td>0.309575</td>\n",
       "      <td>0.860150</td>\n",
       "      <td>0.309954</td>\n",
       "      <td>-0.068060</td>\n",
       "      <td>325.0</td>\n",
       "      <td>355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267610</th>\n",
       "      <td>2.916749</td>\n",
       "      <td>3.445094</td>\n",
       "      <td>-2.175482</td>\n",
       "      <td>-0.509826</td>\n",
       "      <td>0.237448</td>\n",
       "      <td>-0.840167</td>\n",
       "      <td>-0.692287</td>\n",
       "      <td>-0.272305</td>\n",
       "      <td>0.985726</td>\n",
       "      <td>-1.090237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084235</td>\n",
       "      <td>0.153163</td>\n",
       "      <td>-0.370057</td>\n",
       "      <td>-0.618194</td>\n",
       "      <td>0.248808</td>\n",
       "      <td>0.469278</td>\n",
       "      <td>-0.271069</td>\n",
       "      <td>0.695963</td>\n",
       "      <td>530.0</td>\n",
       "      <td>515.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267611 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0       1.737486 -2.731021  1.003991 -0.749037  0.343120  0.137300 -0.636270   \n",
       "1       4.065744 -2.786316  2.362573  0.100823 -3.157451 -0.773902 -0.613166   \n",
       "2      -1.150287 -4.763248 -2.610161 -2.203354  0.662713 -0.600236 -2.152416   \n",
       "3      -1.861710  4.349421 -0.142349  0.402060 -1.094392  6.480412 -1.559656   \n",
       "4      -4.101982  1.267833  2.009555 -0.822039  0.393089  0.163441  0.653145   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "267606  4.308645  0.943745  0.173160  0.638737 -0.904907 -1.192973  2.258960   \n",
       "267607  4.802572 -2.544687  2.720127  2.083392 -3.946778 -3.476563  0.308218   \n",
       "267608 -2.871135  3.626576  1.294119  0.568461 -0.734781 -0.577068 -0.642085   \n",
       "267609 -0.552108 -1.180191  0.466246  1.071564  2.974300 -0.606148 -1.863909   \n",
       "267610  2.916749  3.445094 -2.175482 -0.509826  0.237448 -0.840167 -0.692287   \n",
       "\n",
       "             PC8       PC9      PC10  ...      PC66      PC67      PC68  \\\n",
       "0       1.110771  0.716239 -0.910031  ... -0.434828  0.097037 -0.538475   \n",
       "1       1.171056 -1.146384  0.220550  ...  1.305885  0.227704  0.129430   \n",
       "2      -0.909347 -0.245220  1.599408  ... -0.407897 -0.104403 -0.314425   \n",
       "3      -0.020604 -1.249346 -1.369321  ...  0.687174 -0.034180  1.320186   \n",
       "4       0.437987  1.574366 -0.093781  ...  0.168010 -0.163811  0.932782   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "267606  0.121981 -1.075240 -1.107762  ...  0.055056 -0.511925  0.015117   \n",
       "267607  0.347116 -1.853717  5.699646  ...  0.450210  0.233386  1.010854   \n",
       "267608  0.240285 -0.551313 -0.299072  ...  0.670614  0.043020  0.249646   \n",
       "267609 -2.358189  1.045945 -0.040700  ... -0.140421 -0.528426  0.756357   \n",
       "267610 -0.272305  0.985726 -1.090237  ...  0.084235  0.153163 -0.370057   \n",
       "\n",
       "            PC69      PC70      PC71      PC72      PC73      x      y  \n",
       "0      -0.391294  0.255213 -0.158027  0.119743  0.068547  425.0  285.0  \n",
       "1       0.120253  0.704566  0.516766 -0.532212  0.085923  575.0  250.0  \n",
       "2       0.310318  0.068415 -0.420980 -0.171047 -0.072021  245.0  230.0  \n",
       "3      -0.667891  0.400407 -0.347652 -0.463541  1.118421  395.0  555.0  \n",
       "4       0.929322 -0.015037 -1.069594 -0.199652 -0.251304  275.0  490.0  \n",
       "...          ...       ...       ...       ...       ...    ...    ...  \n",
       "267606  0.091863 -0.000452  0.407204  0.567554 -0.640532  580.0  390.0  \n",
       "267607 -0.238552  0.103711  1.291865 -0.655887 -0.371007  580.0  285.0  \n",
       "267608  0.551378  0.226044  0.939201 -0.258860 -0.573653  360.0  550.0  \n",
       "267609  0.186438  0.309575  0.860150  0.309954 -0.068060  325.0  355.0  \n",
       "267610 -0.618194  0.248808  0.469278 -0.271069  0.695963  530.0  515.0  \n",
       "\n",
       "[267611 rows x 75 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_otiliers_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the datasets in X and y\n",
    "X_train = df_without_otiliers_pca.loc[:,:'PC60']\n",
    "y = df_without_otiliers_pca[['x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'estimator__n_estimators': [10, 100, 500, 1000],\n",
    "    'estimator__max_depth': [10, 100, 500, None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC51</th>\n",
       "      <th>PC52</th>\n",
       "      <th>PC53</th>\n",
       "      <th>PC54</th>\n",
       "      <th>PC55</th>\n",
       "      <th>PC56</th>\n",
       "      <th>PC57</th>\n",
       "      <th>PC58</th>\n",
       "      <th>PC59</th>\n",
       "      <th>PC60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.737486</td>\n",
       "      <td>-2.731021</td>\n",
       "      <td>1.003991</td>\n",
       "      <td>-0.749037</td>\n",
       "      <td>0.343120</td>\n",
       "      <td>0.137300</td>\n",
       "      <td>-0.636270</td>\n",
       "      <td>1.110771</td>\n",
       "      <td>0.716239</td>\n",
       "      <td>-0.910031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349431</td>\n",
       "      <td>1.677243</td>\n",
       "      <td>0.045026</td>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.644882</td>\n",
       "      <td>-0.334647</td>\n",
       "      <td>-0.525434</td>\n",
       "      <td>-0.078617</td>\n",
       "      <td>-0.148611</td>\n",
       "      <td>0.553067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.065744</td>\n",
       "      <td>-2.786316</td>\n",
       "      <td>2.362573</td>\n",
       "      <td>0.100823</td>\n",
       "      <td>-3.157451</td>\n",
       "      <td>-0.773902</td>\n",
       "      <td>-0.613166</td>\n",
       "      <td>1.171056</td>\n",
       "      <td>-1.146384</td>\n",
       "      <td>0.220550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331855</td>\n",
       "      <td>-0.110009</td>\n",
       "      <td>-0.802279</td>\n",
       "      <td>1.627539</td>\n",
       "      <td>-2.065427</td>\n",
       "      <td>1.019473</td>\n",
       "      <td>0.366251</td>\n",
       "      <td>1.418207</td>\n",
       "      <td>0.139856</td>\n",
       "      <td>-1.455523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.150287</td>\n",
       "      <td>-4.763248</td>\n",
       "      <td>-2.610161</td>\n",
       "      <td>-2.203354</td>\n",
       "      <td>0.662713</td>\n",
       "      <td>-0.600236</td>\n",
       "      <td>-2.152416</td>\n",
       "      <td>-0.909347</td>\n",
       "      <td>-0.245220</td>\n",
       "      <td>1.599408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342383</td>\n",
       "      <td>-0.123345</td>\n",
       "      <td>0.615184</td>\n",
       "      <td>2.297369</td>\n",
       "      <td>0.338536</td>\n",
       "      <td>-1.170694</td>\n",
       "      <td>0.126134</td>\n",
       "      <td>2.126381</td>\n",
       "      <td>0.759301</td>\n",
       "      <td>-1.156468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.861710</td>\n",
       "      <td>4.349421</td>\n",
       "      <td>-0.142349</td>\n",
       "      <td>0.402060</td>\n",
       "      <td>-1.094392</td>\n",
       "      <td>6.480412</td>\n",
       "      <td>-1.559656</td>\n",
       "      <td>-0.020604</td>\n",
       "      <td>-1.249346</td>\n",
       "      <td>-1.369321</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.408002</td>\n",
       "      <td>0.677241</td>\n",
       "      <td>0.212914</td>\n",
       "      <td>-1.061706</td>\n",
       "      <td>0.611651</td>\n",
       "      <td>1.527259</td>\n",
       "      <td>1.032460</td>\n",
       "      <td>-0.630776</td>\n",
       "      <td>-0.544068</td>\n",
       "      <td>0.245981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.101982</td>\n",
       "      <td>1.267833</td>\n",
       "      <td>2.009555</td>\n",
       "      <td>-0.822039</td>\n",
       "      <td>0.393089</td>\n",
       "      <td>0.163441</td>\n",
       "      <td>0.653145</td>\n",
       "      <td>0.437987</td>\n",
       "      <td>1.574366</td>\n",
       "      <td>-0.093781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271198</td>\n",
       "      <td>-0.397106</td>\n",
       "      <td>0.796185</td>\n",
       "      <td>-0.516639</td>\n",
       "      <td>0.071735</td>\n",
       "      <td>0.195821</td>\n",
       "      <td>1.177861</td>\n",
       "      <td>-0.474617</td>\n",
       "      <td>0.370597</td>\n",
       "      <td>-0.871449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267606</th>\n",
       "      <td>4.308645</td>\n",
       "      <td>0.943745</td>\n",
       "      <td>0.173160</td>\n",
       "      <td>0.638737</td>\n",
       "      <td>-0.904907</td>\n",
       "      <td>-1.192973</td>\n",
       "      <td>2.258960</td>\n",
       "      <td>0.121981</td>\n",
       "      <td>-1.075240</td>\n",
       "      <td>-1.107762</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.353034</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>0.706876</td>\n",
       "      <td>-0.839135</td>\n",
       "      <td>0.818072</td>\n",
       "      <td>1.540475</td>\n",
       "      <td>-1.079323</td>\n",
       "      <td>-0.081620</td>\n",
       "      <td>0.514737</td>\n",
       "      <td>-0.517755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267607</th>\n",
       "      <td>4.802572</td>\n",
       "      <td>-2.544687</td>\n",
       "      <td>2.720127</td>\n",
       "      <td>2.083392</td>\n",
       "      <td>-3.946778</td>\n",
       "      <td>-3.476563</td>\n",
       "      <td>0.308218</td>\n",
       "      <td>0.347116</td>\n",
       "      <td>-1.853717</td>\n",
       "      <td>5.699646</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.313277</td>\n",
       "      <td>0.566303</td>\n",
       "      <td>-0.120462</td>\n",
       "      <td>0.978422</td>\n",
       "      <td>-0.904991</td>\n",
       "      <td>3.412659</td>\n",
       "      <td>7.444010</td>\n",
       "      <td>0.410137</td>\n",
       "      <td>-1.003981</td>\n",
       "      <td>0.224835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267608</th>\n",
       "      <td>-2.871135</td>\n",
       "      <td>3.626576</td>\n",
       "      <td>1.294119</td>\n",
       "      <td>0.568461</td>\n",
       "      <td>-0.734781</td>\n",
       "      <td>-0.577068</td>\n",
       "      <td>-0.642085</td>\n",
       "      <td>0.240285</td>\n",
       "      <td>-0.551313</td>\n",
       "      <td>-0.299072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017882</td>\n",
       "      <td>-0.066101</td>\n",
       "      <td>-0.611579</td>\n",
       "      <td>1.154850</td>\n",
       "      <td>-0.702385</td>\n",
       "      <td>-0.290137</td>\n",
       "      <td>-0.602963</td>\n",
       "      <td>0.142659</td>\n",
       "      <td>-2.428341</td>\n",
       "      <td>0.337367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267609</th>\n",
       "      <td>-0.552108</td>\n",
       "      <td>-1.180191</td>\n",
       "      <td>0.466246</td>\n",
       "      <td>1.071564</td>\n",
       "      <td>2.974300</td>\n",
       "      <td>-0.606148</td>\n",
       "      <td>-1.863909</td>\n",
       "      <td>-2.358189</td>\n",
       "      <td>1.045945</td>\n",
       "      <td>-0.040700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219699</td>\n",
       "      <td>-0.944044</td>\n",
       "      <td>0.452017</td>\n",
       "      <td>-0.432191</td>\n",
       "      <td>-0.268575</td>\n",
       "      <td>-0.658749</td>\n",
       "      <td>-1.058882</td>\n",
       "      <td>-0.977207</td>\n",
       "      <td>0.704415</td>\n",
       "      <td>-0.824268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267610</th>\n",
       "      <td>2.916749</td>\n",
       "      <td>3.445094</td>\n",
       "      <td>-2.175482</td>\n",
       "      <td>-0.509826</td>\n",
       "      <td>0.237448</td>\n",
       "      <td>-0.840167</td>\n",
       "      <td>-0.692287</td>\n",
       "      <td>-0.272305</td>\n",
       "      <td>0.985726</td>\n",
       "      <td>-1.090237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.825577</td>\n",
       "      <td>-0.764242</td>\n",
       "      <td>-0.115156</td>\n",
       "      <td>-0.483159</td>\n",
       "      <td>2.233093</td>\n",
       "      <td>1.431852</td>\n",
       "      <td>0.293697</td>\n",
       "      <td>-0.211466</td>\n",
       "      <td>0.192559</td>\n",
       "      <td>1.077593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267611 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0       1.737486 -2.731021  1.003991 -0.749037  0.343120  0.137300 -0.636270   \n",
       "1       4.065744 -2.786316  2.362573  0.100823 -3.157451 -0.773902 -0.613166   \n",
       "2      -1.150287 -4.763248 -2.610161 -2.203354  0.662713 -0.600236 -2.152416   \n",
       "3      -1.861710  4.349421 -0.142349  0.402060 -1.094392  6.480412 -1.559656   \n",
       "4      -4.101982  1.267833  2.009555 -0.822039  0.393089  0.163441  0.653145   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "267606  4.308645  0.943745  0.173160  0.638737 -0.904907 -1.192973  2.258960   \n",
       "267607  4.802572 -2.544687  2.720127  2.083392 -3.946778 -3.476563  0.308218   \n",
       "267608 -2.871135  3.626576  1.294119  0.568461 -0.734781 -0.577068 -0.642085   \n",
       "267609 -0.552108 -1.180191  0.466246  1.071564  2.974300 -0.606148 -1.863909   \n",
       "267610  2.916749  3.445094 -2.175482 -0.509826  0.237448 -0.840167 -0.692287   \n",
       "\n",
       "             PC8       PC9      PC10  ...      PC51      PC52      PC53  \\\n",
       "0       1.110771  0.716239 -0.910031  ...  0.349431  1.677243  0.045026   \n",
       "1       1.171056 -1.146384  0.220550  ... -0.331855 -0.110009 -0.802279   \n",
       "2      -0.909347 -0.245220  1.599408  ...  0.342383 -0.123345  0.615184   \n",
       "3      -0.020604 -1.249346 -1.369321  ... -1.408002  0.677241  0.212914   \n",
       "4       0.437987  1.574366 -0.093781  ... -0.271198 -0.397106  0.796185   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "267606  0.121981 -1.075240 -1.107762  ... -1.353034  0.674847  0.706876   \n",
       "267607  0.347116 -1.853717  5.699646  ... -1.313277  0.566303 -0.120462   \n",
       "267608  0.240285 -0.551313 -0.299072  ... -0.017882 -0.066101 -0.611579   \n",
       "267609 -2.358189  1.045945 -0.040700  ...  0.219699 -0.944044  0.452017   \n",
       "267610 -0.272305  0.985726 -1.090237  ... -0.825577 -0.764242 -0.115156   \n",
       "\n",
       "            PC54      PC55      PC56      PC57      PC58      PC59      PC60  \n",
       "0       0.058968  0.644882 -0.334647 -0.525434 -0.078617 -0.148611  0.553067  \n",
       "1       1.627539 -2.065427  1.019473  0.366251  1.418207  0.139856 -1.455523  \n",
       "2       2.297369  0.338536 -1.170694  0.126134  2.126381  0.759301 -1.156468  \n",
       "3      -1.061706  0.611651  1.527259  1.032460 -0.630776 -0.544068  0.245981  \n",
       "4      -0.516639  0.071735  0.195821  1.177861 -0.474617  0.370597 -0.871449  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "267606 -0.839135  0.818072  1.540475 -1.079323 -0.081620  0.514737 -0.517755  \n",
       "267607  0.978422 -0.904991  3.412659  7.444010  0.410137 -1.003981  0.224835  \n",
       "267608  1.154850 -0.702385 -0.290137 -0.602963  0.142659 -2.428341  0.337367  \n",
       "267609 -0.432191 -0.268575 -0.658749 -1.058882 -0.977207  0.704415 -0.824268  \n",
       "267610 -0.483159  2.233093  1.431852  0.293697 -0.211466  0.192559  1.077593  \n",
       "\n",
       "[267611 rows x 60 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=RandomForestRegressor(random_state=0))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=RandomForestRegressor(random_state=0))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=0)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=RandomForestRegressor(random_state=0))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiOutputRegressor(RandomForestRegressor(random_state=0)).fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train = MultiOutputRegressor(RandomForestRegressor())\n",
    "rf_grid_search = GridSearchCV(rf_train, param_grid_rf, cv=5, scoring=custom_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53523, 53523) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53523, 53523) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\UdeA\\Lab\\Final project\\functions.py\", line 5, in custom_error\n",
      "    return np.mean(np.diag(euclidean_distances(y_test, y_pred)))\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 328, in euclidean_distances\n",
      "    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 369, in _euclidean_distances\n",
      "    distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n",
      "  File \"c:\\Users\\alejandrs\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 152, in safe_sparse_dot\n",
      "    ret = a @ b\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 21.3 GiB for an array with shape (53522, 53522) and data type float64\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred=rf_grid_search.fit(X_train, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
