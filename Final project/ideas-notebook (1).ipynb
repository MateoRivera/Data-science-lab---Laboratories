{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7427002,"sourceType":"datasetVersion","datasetId":4320688}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For handling data\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display  # to display variables in a \"nice\" way\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# For Outlier Detection\nfrom sklearn.ensemble import IsolationForest\n\n# For Dimensionality Reduction\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# For evaluating the performance of the preprocessing\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom shutil import copyfile\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nimport keras\nfrom keras import layers\nfrom keras.layers import Activation, Dense\n\n#For download  and upload\nimport joblib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-25T18:17:25.173187Z","iopub.execute_input":"2024-01-25T18:17:25.174439Z","iopub.status.idle":"2024-01-25T18:17:25.185986Z","shell.execute_reply.started":"2024-01-25T18:17:25.174385Z","shell.execute_reply":"2024-01-25T18:17:25.184654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"/kaggle/input/dsl-winter-project-2024/functions.py\", dst = \"/kaggle/working/functions.py\")\n\n# import all our functions\nfrom functions import mean_euclidean_distance_error","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:17:29.272291Z","iopub.execute_input":"2024-01-25T18:17:29.272740Z","iopub.status.idle":"2024-01-25T18:17:29.294523Z","shell.execute_reply.started":"2024-01-25T18:17:29.272702Z","shell.execute_reply":"2024-01-25T18:17:29.293675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/dsl-winter-project-2024/development.csv'\ndf = pd.read_csv(path)\ndf2=df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:17:31.330973Z","iopub.execute_input":"2024-01-25T18:17:31.331402Z","iopub.status.idle":"2024-01-25T18:17:51.450585Z","shell.execute_reply.started":"2024-01-25T18:17:31.331367Z","shell.execute_reply":"2024-01-25T18:17:51.449618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path2='/kaggle/input/dsl-winter-project-2024/evaluation.csv'\neval=pd.read_csv(path2, index_col='Id')","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:11.173184Z","iopub.execute_input":"2024-01-25T18:18:11.173776Z","iopub.status.idle":"2024-01-25T18:18:18.977127Z","shell.execute_reply.started":"2024-01-25T18:18:11.173731Z","shell.execute_reply":"2024-01-25T18:18:18.975734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sub=eval\nX_sub","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:21.522591Z","iopub.execute_input":"2024-01-25T18:18:21.523081Z","iopub.status.idle":"2024-01-25T18:18:21.581031Z","shell.execute_reply.started":"2024-01-25T18:18:21.523042Z","shell.execute_reply":"2024-01-25T18:18:21.579713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's split the data into X and y (We have two variables to predict: x and y)\nX = df2.loc[:, 'pmax[0]':]\ny = df2[['x', 'y']]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:24.911226Z","iopub.execute_input":"2024-01-25T18:18:24.911693Z","iopub.status.idle":"2024-01-25T18:18:24.922306Z","shell.execute_reply.started":"2024-01-25T18:18:24.911655Z","shell.execute_reply":"2024-01-25T18:18:24.920461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the Feature with the names of the columns to work later\nFeatures=np.array(X.columns)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:35.113350Z","iopub.execute_input":"2024-01-25T18:18:35.113809Z","iopub.status.idle":"2024-01-25T18:18:35.120214Z","shell.execute_reply.started":"2024-01-25T18:18:35.113765Z","shell.execute_reply":"2024-01-25T18:18:35.118561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Let's split X, y into train_val and test sets\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n\n# Let's split X_train_val and y_train_val into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=8)\n\n# Let's reset the indices\nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:38.359712Z","iopub.execute_input":"2024-01-25T18:18:38.360180Z","iopub.status.idle":"2024-01-25T18:18:39.336587Z","shell.execute_reply.started":"2024-01-25T18:18:38.360140Z","shell.execute_reply":"2024-01-25T18:18:39.335125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Idea 2: \nRemove t.max","metadata":{}},{"cell_type":"code","source":"mask=[i[0]!='t' for i in Features]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T16:35:27.242983Z","iopub.execute_input":"2024-01-25T16:35:27.244234Z","iopub.status.idle":"2024-01-25T16:35:27.248635Z","shell.execute_reply.started":"2024-01-25T16:35:27.244198Z","shell.execute_reply":"2024-01-25T16:35:27.247830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features=Features[mask] #Update the Features without the tmax's","metadata":{"execution":{"iopub.status.busy":"2024-01-25T16:35:33.994383Z","iopub.execute_input":"2024-01-25T16:35:33.994766Z","iopub.status.idle":"2024-01-25T16:35:33.999610Z","shell.execute_reply.started":"2024-01-25T16:35:33.994738Z","shell.execute_reply":"2024-01-25T16:35:33.998478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Update of the features and X\n\nX = df2.loc[:,Features]\n\n\"\"\"# Let's split X, y into train_val and test sets\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n\n# Let's split X_train_val and y_train_val into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=8)\n\n# Let's reset the indices\nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T16:35:36.956671Z","iopub.execute_input":"2024-01-25T16:35:36.957128Z","iopub.status.idle":"2024-01-25T16:35:37.470265Z","shell.execute_reply.started":"2024-01-25T16:35:36.957080Z","shell.execute_reply":"2024-01-25T16:35:37.469053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nrf_param_grid = {\n    'estimator__n_estimators': [10, 100, 500, 1000, 2000],\n    'estimator__bootstrap': [True, False],\n}\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nrf_grid_search = GridSearchCV(MultiOutputRegressor(RandomForestRegressor(random_state=8), n_jobs=-1), rf_param_grid, scoring='neg_mean_squared_error')\nrf_grid_search.fit(X_train, y_train)\nprint(rf_grid_search.best_params_, mean_euclidean_distance_error(rf_grid_search, X_valid, y_valid))\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Idea 3:\nDue to pmax and negpmax are the most important features of the model we could try to get the rank of the waves and score its importance in the model ","metadata":{}},{"cell_type":"code","source":"df3=df2.copy()","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:52.302875Z","iopub.execute_input":"2024-01-25T18:18:52.303356Z","iopub.status.idle":"2024-01-25T18:18:52.479369Z","shell.execute_reply.started":"2024-01-25T18:18:52.303318Z","shell.execute_reply":"2024-01-25T18:18:52.477718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rank=np.zeros([18,len(df3['x'])])\nrank_e=np.zeros([18,len(X_sub.iloc[:,1])])","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:54.150830Z","iopub.execute_input":"2024-01-25T18:18:54.151345Z","iopub.status.idle":"2024-01-25T18:18:54.163574Z","shell.execute_reply.started":"2024-01-25T18:18:54.151300Z","shell.execute_reply":"2024-01-25T18:18:54.162020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" for i in range(18):\n        rank[i]=df3[f'pmax[{i}]']-df3[f'negpmax[{i}]']\n        df3[f'rank[{i}]']=rank[i]\n        df3.drop([f'pmax[{i}]',f'negpmax[{i}]'],axis=1, inplace=True)\n        \n        \"\"\"Prerpocessing X_val\"\"\"\n        \n        rank_e[i]=X_sub[f'pmax[{i}]']-X_sub[f'negpmax[{i}]']\n        X_sub[f'rank[{i}]']=rank_e[i]\n        X_sub.drop([f'pmax[{i}]',f'negpmax[{i}]'],axis=1, inplace=True)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:18:57.161894Z","iopub.execute_input":"2024-01-25T18:18:57.162975Z","iopub.status.idle":"2024-01-25T18:19:00.392799Z","shell.execute_reply.started":"2024-01-25T18:18:57.162924Z","shell.execute_reply":"2024-01-25T18:19:00.391605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's split the data into X and y (We have two variables to predict: x and y)\nX = df3.loc[:, 'area[0]':]\ny = df3[['x', 'y']]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:19:02.150129Z","iopub.execute_input":"2024-01-25T18:19:02.150581Z","iopub.status.idle":"2024-01-25T18:19:02.276786Z","shell.execute_reply.started":"2024-01-25T18:19:02.150544Z","shell.execute_reply":"2024-01-25T18:19:02.275869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the Feature with the names of the columns to work later\nFeatures=np.array(X.columns)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:19:08.309536Z","iopub.execute_input":"2024-01-25T18:19:08.309972Z","iopub.status.idle":"2024-01-25T18:19:08.315392Z","shell.execute_reply.started":"2024-01-25T18:19:08.309937Z","shell.execute_reply":"2024-01-25T18:19:08.314169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Let's split X, y into train_val and test sets\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n\n# Let's split X_train_val and y_train_val into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=8)\n\n# Let's reset the indices\nX_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T14:24:40.219854Z","iopub.execute_input":"2024-01-25T14:24:40.220285Z","iopub.status.idle":"2024-01-25T14:24:40.808407Z","shell.execute_reply.started":"2024-01-25T14:24:40.220252Z","shell.execute_reply":"2024-01-25T14:24:40.807290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"rf = MultiOutputRegressor(RandomForestRegressor(random_state=8), n_jobs=-1)\nrf.fit(X_train, y_train)\nscore=(mean_euclidean_distance_error(rf, X_val, y_val.values))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T09:26:30.045602Z","iopub.execute_input":"2024-01-20T09:26:30.046082Z","iopub.status.idle":"2024-01-20T10:04:12.811752Z","shell.execute_reply.started":"2024-01-20T09:26:30.046045Z","shell.execute_reply":"2024-01-20T10:04:12.809838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"(sorted(zip(Features, rf.estimators_[0].feature_importances_), key=lambda x: x[1],reverse=True))\n#The Feature importance to the y1 value : x\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T10:18:42.357466Z","iopub.execute_input":"2024-01-20T10:18:42.357951Z","iopub.status.idle":"2024-01-20T10:18:42.501345Z","shell.execute_reply.started":"2024-01-20T10:18:42.357915Z","shell.execute_reply":"2024-01-20T10:18:42.500379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"(sorted(zip(Features, rf.estimators_[1].feature_importances_), key=lambda x: x[1],reverse=True))\n#The Feature importance to the y1 value : x\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T10:18:55.784530Z","iopub.execute_input":"2024-01-20T10:18:55.786132Z","iopub.status.idle":"2024-01-20T10:18:56.111007Z","shell.execute_reply.started":"2024-01-20T10:18:55.786085Z","shell.execute_reply":"2024-01-20T10:18:56.109763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Idea 4: \nIdea 2 and 3 together","metadata":{}},{"cell_type":"code","source":"mask=[i[0]!='t' for i in Features]\nFeatures1=Features[mask] #Update the Features without the tmax's\n#y is the same than in the 2nd idea \n#X must be uploaded \nX1 = df3.loc[:,Features]\nX_sub=X_sub.loc[:,Features]","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:19:32.210210Z","iopub.execute_input":"2024-01-25T18:19:32.210670Z","iopub.status.idle":"2024-01-25T18:19:32.506943Z","shell.execute_reply.started":"2024-01-25T18:19:32.210637Z","shell.execute_reply":"2024-01-25T18:19:32.505717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Let's split X, y into train_val and test sets\nX_train_val1, X_test1, y_train_val1, y_test1 = train_test_split(X1, y, test_size=0.3, random_state=8)\n\n# Let's split X_train_val and y_train_val into train and validation sets\nX_train1, X_val1, y_train1, y_val1 = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=8)\n\n# Let's reset the indices\nX_train1.reset_index(drop=True, inplace=True)\nX_val1.reset_index(drop=True, inplace=True)\nX_test1.reset_index(drop=True, inplace=True)\n\ny_train1.reset_index(drop=True, inplace=True)\ny_val1.reset_index(drop=True, inplace=True)\ny_test1.reset_index(drop=True, inplace=True)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T16:36:30.512873Z","iopub.execute_input":"2024-01-25T16:36:30.513297Z","iopub.status.idle":"2024-01-25T16:36:31.131211Z","shell.execute_reply.started":"2024-01-25T16:36:30.513250Z","shell.execute_reply":"2024-01-25T16:36:31.130220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params={'n_estimators': 600,\n  'max_features': 'sqrt',\n  'min_samples_split': 2,\n  'bootstrap': False,\n  'criterion': 'poisson'}","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:19:42.373336Z","iopub.execute_input":"2024-01-25T18:19:42.374338Z","iopub.status.idle":"2024-01-25T18:19:42.381956Z","shell.execute_reply.started":"2024-01-25T18:19:42.374275Z","shell.execute_reply":"2024-01-25T18:19:42.380258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"rf1 = MultiOutputRegressor(RandomForestRegressor(**params), n_jobs=-1)\nrf1.fit(X_train1, y_train1)\nscore1=(mean_euclidean_distance_error(rf1, X_val1, y_val1.values))\ny_sub1=rf1.predict(X_sub)\n\n#Improve of the metric\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T17:42:54.525535Z","iopub.execute_input":"2024-01-20T17:42:54.526627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Idea 5: \nWe are gonna try to use the idea 4 without ouliers","metadata":{}},{"cell_type":"code","source":"isolation_forest = IsolationForest(n_estimators=1000)\n#X_train1['is_outlier'] = isolation_forest.fit_predict(X_train) # fit_predict returns -1 for outliers and 1 for inliers\n","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:20:01.990851Z","iopub.execute_input":"2024-01-25T18:20:01.991322Z","iopub.status.idle":"2024-01-25T18:20:01.996929Z","shell.execute_reply.started":"2024-01-25T18:20:01.991287Z","shell.execute_reply":"2024-01-25T18:20:01.995792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T14:32:42.014686Z","iopub.execute_input":"2024-01-25T14:32:42.015588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"mask_is_outlier = X_train1['is_outlier'] == -1\nX_train_without_outliers, y_train_without_outliers = X_train1[~mask_is_outlier], y_train1[~mask_is_outlier]\ndel(X_train_without_outliers['is_outlier'])\ndel(X_train1['is_outlier'])\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-25T14:28:25.744588Z","iopub.execute_input":"2024-01-25T14:28:25.745015Z","iopub.status.idle":"2024-01-25T14:28:25.856171Z","shell.execute_reply.started":"2024-01-25T14:28:25.744979Z","shell.execute_reply":"2024-01-25T14:28:25.854857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1['is_outlier']=isolation_forest.fit_predict(X1)\nmask_is_outlier = X1['is_outlier'] == -1\nX_train_without_outliers, y_train_without_outliers = X1[~mask_is_outlier], y[~mask_is_outlier]\ndel(X_train_without_outliers['is_outlier'])\n\n# Let's reset the index\nX_train_without_outliers.reset_index(drop=True, inplace=True)\ny_train_without_outliers.reset_index(drop=True, inplace=True)\nrf2 = MultiOutputRegressor(RandomForestRegressor(**params))\nrf2.fit(X_train_without_outliers, y_train_without_outliers)\n\"\"\"score2=(mean_euclidean_distance_error(rf2, X_val1, y_val1.values))\"\"\"\ny_sub2=rf2.predict(X_sub)\njoblib.dump(y_sub2,'sub_F')\ny_sub2","metadata":{"execution":{"iopub.status.busy":"2024-01-25T18:20:06.520098Z","iopub.execute_input":"2024-01-25T18:20:06.520624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's reset the index\nX_train_without_outliers.reset_index(drop=True, inplace=True)\ny_train_without_outliers.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-25T14:28:32.257608Z","iopub.execute_input":"2024-01-25T14:28:32.258049Z","iopub.status.idle":"2024-01-25T14:28:32.264182Z","shell.execute_reply.started":"2024-01-25T14:28:32.258015Z","shell.execute_reply":"2024-01-25T14:28:32.263213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_without_outliers","metadata":{"execution":{"iopub.status.busy":"2024-01-20T15:16:36.662662Z","iopub.execute_input":"2024-01-20T15:16:36.663058Z","iopub.status.idle":"2024-01-20T15:16:36.735107Z","shell.execute_reply.started":"2024-01-20T15:16:36.663026Z","shell.execute_reply":"2024-01-20T15:16:36.734119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf2 = MultiOutputRegressor(RandomForestRegressor(**params), n_jobs=-1)\nrf2.fit(X_train_without_outliers, y_train_without_outliers)\n\"\"\"score2=(mean_euclidean_distance_error(rf2, X_val1, y_val1.values))\"\"\"\ny_sub2=rf2.predict(X_sub)\njoblib.dump(y_sub2,'sub_F')\nscore2\n#It has increase","metadata":{"execution":{"iopub.status.busy":"2024-01-20T11:08:02.362565Z","iopub.execute_input":"2024-01-20T11:08:02.363117Z","iopub.status.idle":"2024-01-20T11:39:38.091524Z","shell.execute_reply.started":"2024-01-20T11:08:02.363079Z","shell.execute_reply":"2024-01-20T11:39:38.089829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Idea 6:\nUse polynomial regressor","metadata":{}},{"cell_type":"code","source":"\"\"\"First we are gonna apply a sample to the data in order to be able to run the Next models\nX_s=X_train_without_outliers.sample(n=10000, random_state=8)\nmask=X_s.index\ny_s = y_train_without_outliers[y_train_without_outliers.index.isin(mask)]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T15:49:06.417965Z","iopub.execute_input":"2024-01-20T15:49:06.418485Z","iopub.status.idle":"2024-01-20T15:49:06.448597Z","shell.execute_reply.started":"2024-01-20T15:49:06.418424Z","shell.execute_reply":"2024-01-20T15:49:06.447655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"reg=MultiOutputRegressor(make_pipeline(PolynomialFeatures(2),LinearRegression()), n_jobs=1)\nreg.fit(X_s,y_s)\n#Lasso\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T15:49:09.080714Z","iopub.execute_input":"2024-01-20T15:49:09.081210Z","iopub.status.idle":"2024-01-20T16:04:14.223563Z","shell.execute_reply.started":"2024-01-20T15:49:09.081172Z","shell.execute_reply":"2024-01-20T16:04:14.222326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"y_pred=reg.predict(X_val)\nprint(mean_euclidean_distance_error(reg, X_val, y_val.values))\n#It is not a good idea maybe because to the small sample\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-20T16:21:14.031248Z","iopub.execute_input":"2024-01-20T16:21:14.031687Z","iopub.status.idle":"2024-01-20T16:22:01.124251Z","shell.execute_reply.started":"2024-01-20T16:21:14.031654Z","shell.execute_reply":"2024-01-20T16:22:01.122868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Utilizar una RNN","metadata":{}},{"cell_type":"code","source":"n_inputs,n_outputs=X_train_without_outliers.shape[1],y_train_without_outliers.shape[1]\nmodel = keras.Sequential()\nmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\nmodel.add(Dense(n_outputs))\nmodel.compile(loss='mae', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2024-01-25T06:13:25.513016Z","iopub.execute_input":"2024-01-25T06:13:25.513525Z","iopub.status.idle":"2024-01-25T06:13:26.416814Z","shell.execute_reply.started":"2024-01-25T06:13:25.513486Z","shell.execute_reply":"2024-01-25T06:13:26.415275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"model.fit(X_train, y_train, epochs=100)\nscore3=(mean_euclidean_distance_error(model, X_val1, y_val1.values))\ny_sub3=model.predict(X_sub)\"\"\"\nmodel.fit(X_train_without_outliers, y_train_without_outliers, epochs=100)\n\"\"\"score3=(mean_euclidean_distance_error(model, X_val1, y_val1.values))\"\"\"\ny_sub3=model.predict(X_sub)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T08:05:42.543253Z","iopub.execute_input":"2024-01-23T08:05:42.543754Z","iopub.status.idle":"2024-01-23T08:05:42.587696Z","shell.execute_reply.started":"2024-01-23T08:05:42.543707Z","shell.execute_reply":"2024-01-23T08:05:42.586063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(y_sub3,'sub_rnn')\njoblib.dump(y_sub2,'sub_out')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"results={'Rank_wto_o':score2,'RNN':score3}\nresults\"\"\"","metadata":{},"execution_count":null,"outputs":[]}]}