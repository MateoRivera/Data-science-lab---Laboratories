# -*- coding: utf-8 -*-
"""lab2MateoParallel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pgkjeg5VV42YCbSsxvQMm_YpoxR_5_Du

# Exercise 2
"""

import pandas as pd
import numpy as np
import multiprocessing as mp
import string
import time


def tokenize(docs):
    """Compute the tokens for each document.
    Input: a list of strings. Each item is a document to tokenize.
    Output: a list of lists. Each item is a list containing the tokens of the
    relative document.
    """
    tokens = []
    for doc, label in docs:
        for punct in string.punctuation:
            doc = doc.replace(punct, " ")
        split_doc = [ token.lower() for token in doc.split(" ") if token ]
        tokens.append(split_doc)
    return tokens

def TF(tokens):
    unique_counts_tokens = np.unique(tokens, return_counts=True)
    return { token:token_count for token, token_count in zip(unique_counts_tokens[0], unique_counts_tokens[1]) }

"""4"""

def IDF_t(N, DF_t):
    return np.log(N/DF_t)

def DF_t(token, documents_tokenized):
    return sum([ 1 if token in document else 0 for document in documents_tokenized ])

def DF_ts(tokens, documents_tokenized):
    return { token:DF_t(token, documents_tokenized) for token in tokens }

def DF_ts_parallel(tokens_and_documents_tokenized):
    return { token:DF_t(token, tokens_and_documents_tokenized[1]) for token in tokens_and_documents_tokenized[0] }

def split_list_into_subsets(input_list, subset_size):
    subsets = []
    for i in range(0, len(input_list), subset_size):
        subset = input_list[i:i + subset_size]
        subsets.append(subset)

    return subsets        

if __name__ == "__main__":
    start = time.time()
    """1. Load the IMDb dataset as a list of lists."""
    IMDb_reviews_URL = 'https://raw.githubusercontent.com/dbdmg/data-science-lab/master/datasets/aclimdb_reviews_train.txt'
    IMDb_reviews_dataset = pd.read_csv(IMDb_reviews_URL).values.tolist()

    """2. Apply the tokenization function listed below to your reviews. Please refer to the functionâ€™s docstring 1
    for the input and output parameters. The tokenization procedure splits each comment in tokens (i.e.
    separate words).
    """
    IMDb_reviews_documents_tokenized = tokenize(IMDb_reviews_dataset)

    """3"""
    IMDb_reviews_TF = [ TF(document_tokenized) for document_tokenized in IMDb_reviews_documents_tokenized ]

    """4"""
    """(a) Compute the DF (Document frecuency: is the document frequency of a token, i.e. the number
    of documents in which t appears at least once.) for all of your tokens
    """
    IMDb_reviews_tokens_set_list = np.array(list(set([ token for document_tokenized in IMDb_reviews_documents_tokenized for token in document_tokenized ])))

    pair_tokens_and_documents_tokenized = ([tokens_sub_set, IMDb_reviews_documents_tokenized] for tokens_sub_set in split_list_into_subsets(IMDb_reviews_tokens_set_list, 4))

    print("Parallelizing...")
    pool = mp.Pool(processes=4)

    IMDb_reviews_DF_t = pool.map(DF_ts_parallel, pair_tokens_and_documents_tokenized)
    pool.close()

    print("Parallelizing... Done")

    print("Merging...")
    IMDb_reviews_DF_t_merge = { token: sub_set_DF[token]  for sub_set_DF in IMDb_reviews_DF_t for token in sub_set_DF }
    
    # #IMDb_reviews_DF_t = DF_ts_parallel(IMDb_reviews_tokens_set_list, IMDb_reviews_documents_tokenized)

    # """(b) Compute the IDF for all of your tokens;"""
    # N = len(IMDb_reviews_dataset)
    # IMDb_reviews_IDF_t = { token: IDF_t(N, IMDb_reviews_DF_t[token]) for token in IMDb_reviews_DF_t}

    # """(c) Try to sort the IDF values in ascending order. Which tokens (i.e. words) came to the top? Can
    # you figure out why?
    # R/ Yes, it's because we can find the top tokens in most of the documents and it's due to they have a low $IDF_{t}$ and this value is greater in stranger tokens.
    # """
    # print(f"IDF_t sorted ascending: {sorted(IMDb_reviews_IDF_t, key=lambda k: IMDb_reviews_IDF_t[k])}")

    # """5"""

    # TF_IDF = list()
    # for TF_document_i in IMDb_reviews_TF:
    #     TF_IDF_t_d = { token: TF_document_i[token] * IMDb_reviews_IDF_t[token] for token in TF_document_i}
    #     TF_IDF.append(TF_IDF_t_d)

    import json
    json_data = json.dumps(IMDb_reviews_DF_t_merge)
    with open('IMDb_reviews_DF_t3.json', 'w') as f:
        f.write(json_data)
    
    print(f"Time elapsed with parallelization: {time.time() - start} seconds")

